<!DOCTYPE html>
<html>
<head>
<style>
body { font-family: Arial; }
</style>
<script>

function deleteAll()
{
        var xmlhttp;
        if (window.XMLHttpRequest)
        {
                xmlhttp = new XMLHttpRequest();
        }
        else
        {
                xmlhttp = new ActiveXObject("Microsoft.XMLHTTP");
        }

        xmlhttp.open("DELETE","./JCrawlerServlet",false);
        xmlhttp.send();
}

function postAJAX()
{
	var xmlhttp;
	if (window.XMLHttpRequest)
 	{
		xmlhttp=new XMLHttpRequest();
	}
	else
	{
		xmlhttp=new ActiveXObject("Microsoft.XMLHTTP");
	}
	
	xmlhttp.open("POST","./JCrawlerServlet",false);
	xmlhttp.setRequestHeader("Content-type", "application/x-www-form-urlencoded");
	xmlhttp.send("crawl_url=" + document.getElementById('urlkey').value);

}

function getAJAX( getKey)
{
        var xmlhttp;
        if (window.XMLHttpRequest)
        {
                xmlhttp = new XMLHttpRequest();
        }
        else
        {
                xmlhttp = new ActiveXObject("Microsoft.XMLHTTP");
        }
        
        xmlhttp.onreadystatechange = function()
        {
                if (xmlhttp.readyState==4 && xmlhttp.status==200)
                {
                        document.getElementById("urls").innerHTML=xmlhttp.responseText;
                }
        }

        xmlhttp.open("GET","./JCrawlerServlet?urlkey=" + getKey,true);
        xmlhttp.send();

	return false;

}

</script>
<title>JCrawler</title>
</head>
<body onload="getAJAX(0);">
<a href="/">HOME</a><p/>
<b>Crawl a URL</b><br />
<input type="text" id="urlkey" value="http://" />
<button type="button" onclick="postAJAX();getAJAX(0);">Submit</button>
<br />
<button type="button" onclick="deleteAll();getAJAX(0);">Reset</button>
<br />
<hr />
<b>Database-stored URL crawls</b> (click links, if any, to see results of the crawl) <p />
<div id="urls">
</div>
<p />
<b>About</b> <br />

This tool is a simple Java-based web crawler. Enter a complete url (beginning with "http://") and that page will be crawled, its links will be stored in the database and any links it finds which are also in that domain will be crawled as well, recursively. It assumes that an absolute URL found on a web page is external to that domain and therefore does not crawl it. 
<p />
<p>
Front page: HTML with JavaScript and AJAX
<br />
Back end: Java Servlet via Tomcat 7, MySQL 5.5.34, Linux (Ubuntu 13.04) on AWS/EC2
<p />
<a href="https://github.com/scottdjohnson/JCrawler">Source code on gituhb</a>
<p />
<a href="javadoc">JavaDoc</a>
</body>
</html>
